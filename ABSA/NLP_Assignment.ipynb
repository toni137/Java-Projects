{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLCRzK96crTr"
      },
      "source": [
        "# Sentence-level ABSA\n",
        "Given a sentence, we'll need to identify and annotate the aspect category and the sentiment polarity towards the given category.\n",
        "\n",
        "Because each sentence can belong to multiple categories at the same time, I have decided to implement a multi-label classification model. The idea behind this is to transform our multi-label problem into a multi-class problem, where my classes are not mutually exclusive.\n",
        "\n",
        "In order to create only one model for both the category and the sentiment, I am going to add the polarity to each tag. This might not be the best approach to dealing with this task as it increases the dimensionality of our data significantly (from 198 labels to 594).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vewvHSfktO9E",
        "outputId": "d836e805-9a33-40a4-ca16-10deeb525377"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting scikit-multilearn\n",
            "  Downloading scikit_multilearn-0.2.0-py3-none-any.whl (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.4/89.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scikit-multilearn\n",
            "Successfully installed scikit-multilearn-0.2.0\n"
          ]
        }
      ],
      "source": [
        "%pip install scikit-multilearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0hcHG1as0ew",
        "outputId": "72c51724-b6d2-4d5e-e563-920c80dbb03a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import PorterStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOxe_nOIJq2M"
      },
      "outputs": [],
      "source": [
        "from skmultilearn.problem_transform import BinaryRelevance\n",
        "from skmultilearn.problem_transform import ClassifierChain\n",
        "from skmultilearn.problem_transform import LabelPowerset\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.utils import class_weight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZQxXRPKdQP_"
      },
      "source": [
        "Extracting Data \n",
        "From the given XML files, we'll need the sententces, categories and the polarities of each category. We'll extract those using ElementTrees."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfZf4wKR7MMs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xml.etree.cElementTree as et\n",
        "\n",
        "tree_train=et.parse('Laptops_Train_p1.xml')\n",
        "root_train=tree_train.getroot()\n",
        "\n",
        "tree_test=et.parse('Laptops_Test_p1_gold.xml')\n",
        "root_test=tree_test.getroot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dx-tSLk0eN95"
      },
      "source": [
        "We  generate the categories from the given sets. (Note: I modified HARD_DISK to HARD_DISC as there seems to have been a misspelling in the files provided)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vINjGTv7gwK"
      },
      "outputs": [],
      "source": [
        "entity = [\"LAPTOP\", \"DISPLAY\", \"KEYBOARD\", \"MOUSE\", \"MOTHERBOARD\", \"CPU\", \"FANS_COOLING\", \"PORTS\", \"MEMORY\", \"POWER_SUPPLY\" , \"OPTICAL_DRIVES\", \"BATTERY\", \"GRAPHICS\", \"HARD_DISC\", \"MULTIMEDIA_DEVICES\", \"HARDWARE\", \"SOFTWARE\", \"OS\", \"WARRANTY\", \"SHIPPING\", \"SUPPORT\", \"COMPANY\"]\n",
        "attributes = [\"GENERAL\", \"PRICE\", \"QUALITY\", \"DESIGN_FEATURES\", \"OPERATION_PERFORMANCE\", \"USABILITY\", \"PORTABILITY\", \"CONNECTIVITY\", \"MISCELLANEOUS\"]\n",
        "polarity = [\"positive\", \"negative\", \"neutral\"]\n",
        "Cat = []\n",
        "for e in entity:\n",
        "  for a in attributes:\n",
        "    for p in polarity:\n",
        "      Cat.append(e+'#'+a+'#'+p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2T1C5ul6efWV"
      },
      "source": [
        "I don't need the sententences which don't belong to any category, so I'll ignore those.\n",
        "\n",
        "I create a row for each label, where the first element is the name of the tag, then for each sentence I add 0 if that sentence doesn't have that tag, and 1 if it does."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oChHx22gGR32"
      },
      "outputs": [],
      "source": [
        "def extract_data(root):\n",
        "  \n",
        "  row_text = []\n",
        "  Categories = []\n",
        "  for i in range(len(Cat)):\n",
        "    row_text.append([Cat[i]]) #create a row for each category\n",
        "\n",
        "  Text = []\n",
        "  for sentence in root.iter('sentence'):\n",
        "    category = str(sentence.find('Opinions'))\n",
        "    if category != 'None': #ignore sentences with no category\n",
        "      Text.append(sentence.find('text').text)\n",
        "      for index in range(len(Cat)):\n",
        "        row_text[index].append(0) #add 0 for all categories for current sentence\n",
        "      for i in sentence.iter('Opinion'): #iterate over all existing categories of the current sentence\n",
        "       pol = str(i.get('polarity'))\n",
        "       cat = str(i.get('category'))\n",
        "       entity = cat + '#' + pol\n",
        "       for j in range(len(Cat)):\n",
        "         if entity == Cat[j]:\n",
        "            row_text[j].pop()\n",
        "            row_text[j].append(1) #replace the 0 with 1 for each category of the current sentence\n",
        "\n",
        "  row_text.sort() #sort the tags so we'll have the same order in both data sets\n",
        "  return Text , row_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qV5nPIxG-OD"
      },
      "outputs": [],
      "source": [
        "Text_train = []\n",
        "row_text_train = []\n",
        "Text_train , row_text_train = extract_data(root_train)\n",
        "Text_test = []\n",
        "row_text_test = []\n",
        "Text_test , row_text_test = extract_data(root_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ThHppAjeZtN"
      },
      "source": [
        "Because we can't make predictions about a tag that doesn't occur in our training set and there's no point in training our model on tags that we won't make predictions about, we'll ignore those.\n",
        "\n",
        "Therefore, in order to minimaze the dimensionality of our data, we'll remove the tags which have no ocurences in neither the training set nor the testing set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10SRv2vBSOIu",
        "outputId": "3aba7284-1e8a-4757-9dd4-dc22cd2d3f4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "105\n"
          ]
        }
      ],
      "source": [
        "\n",
        "i = 0\n",
        "while i < len(row_text_test):\n",
        "  if {0} == set(row_text_test[i][1:]) or {0} == set(row_text_train[i][1:]):\n",
        "    row_text_test.pop(i)\n",
        "    row_text_train.pop(i)\n",
        "    i = i - 1 \n",
        "  i += 1\n",
        "print(len(row_text_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMr5_ZK-lxSU"
      },
      "source": [
        "As you can see only 105 out of a total of 594 tags have at least one occurence in both data sets which is a significant reduction in our data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOcM3VCQfFOT"
      },
      "source": [
        "Pre-processing our data\n",
        "We'll do the basic steps of pre-processing our data:\n",
        " 1. words are lower case\n",
        " 2. tokenize\n",
        " 3. stop-word removal\n",
        " 4. punctuation and non-alpha character removal\n",
        " 5. Lemmatise the words in the text\n",
        "\n",
        "I chose to use lemmatising in my algorithm as it slightly improves the accuracy of the model, stemming has proven to be too harsh for this task.\n",
        "\n",
        " Moreover, since negation has a powerful impact on the meaning of our words, we'll also replace the words after a negation with their antonyms. The algortihm for negation uses WordNet to find all the antonyms of each word. It is not always the case that there are any antonyms (for example verbs or pronouns), and in this case the word will not be replaced. If there are antonyms, I'll create a list of all posssible antonyms and pick the one with the highest dissimilarity coeficient.\n",
        " \n",
        " This step can be improved by replacing only the relevant parts of the sentence with antonyms. In my algorithm, I replace everything after the negation (not, never etc.) with their antonyms but it's not always the need for this. In compound sentences, stopwords such as \"but\", \"whereas\", etc. mark the the begining of a new clause in the sentence, which prevents the effect of negation to extend to it. For example: \"I don't like soup but I love vegetables\", the negation \"n't\" affects only the left clause of the sentence. A good method that might refine my algorithm is *conjunction analysis* as conjunctions are the words which link together diffrent clauses in a sentence.\n",
        " Also adding more cases of negation to include misspellings and prefixes such as \"un-\" is the next step that I want to take to improve my algotihm.\n",
        "\n",
        " Another improvement that might increase the accruracy of the model is to replace all synonym words with the same words to reduce the number of words in our dictionary.\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7x1lhkb7uw0"
      },
      "outputs": [],
      "source": [
        "# original code for negation https://gist.github.com/UtkarshRedd/3fbfd354ea7a6f83bd8f9419a27b0543#file-negation_handler-py\n",
        "def Negation(sentence):\t\n",
        "\n",
        "  temp = int(0)\n",
        "  for i in range(len(sentence)):\n",
        "      if sentence[i-1] in ['not',\"n't\", \"never\"]:\n",
        "        for j in range(i, len(sentence)):\n",
        "          antonyms = []\n",
        "          for syn in wordnet.synsets(sentence[j]):\n",
        "              syns = wordnet.synsets(sentence[j])\n",
        "              w1 = syns[0].name()\n",
        "              temp = 0\n",
        "              for l in syn.lemmas():\n",
        "                  if l.antonyms():\n",
        "                      antonyms.append(l.antonyms()[0].name())\n",
        "              max_dissimilarity = 0\n",
        "              for ant in antonyms:\n",
        "                  syns = wordnet.synsets(ant)\n",
        "                  w2 = syns[0].name()\n",
        "                  syns = wordnet.synsets(sentence[j])\n",
        "                  w1 = syns[0].name()\n",
        "                  word1 = wordnet.synset(w1)\n",
        "                  word2 = wordnet.synset(w2)\n",
        "                  if isinstance(word1.wup_similarity(word2), float) or isinstance(word1.wup_similarity(word2), int):\n",
        "                      temp = 1 - word1.wup_similarity(word2)\n",
        "                  if temp>max_dissimilarity:\n",
        "                      max_dissimilarity = temp\n",
        "                      antonym_max = ant\n",
        "                      sentence[j] = antonym_max\n",
        "                      sentence[i-1] = ''\n",
        "  while '' in sentence:\n",
        "      sentence.remove('')\n",
        "  return sentence \n",
        "def remove_stopwords(tknzd_text):\n",
        "  tokens = [] #list of tokens w/o stopwords\n",
        "  for token in tknzd_text:\n",
        "    if token not in stopwords.words('english'):\n",
        "      tokens.append(token)\n",
        "  return tokens\n",
        "\n",
        "def remove_non_alpha(tknzd_text):\n",
        "\n",
        "  alpha_tokens = [] #list of tokens that are only alphabetic \n",
        "  for token in tknzd_text:\n",
        "    if token.isalpha():\n",
        "      alpha_tokens.append(token)\n",
        "  return alpha_tokens\n",
        "\n",
        "def lemmatise(tknzd_text):\n",
        "  lemma_tokens = [] #list of lemmatized tokens\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  for token in tknzd_text:\n",
        "    lemma_tokens.append(lemmatizer.lemmatize(token))\n",
        "  \n",
        "  lemmatized_text = \" \".join(lemma_tokens)\n",
        "  return lemmatized_text\n",
        "\n",
        "def preprocess(tokenized_data):\n",
        "  pp_data = []  #list of preprocessed sms. This is not tokenized text anymore\n",
        "  for tknzd_sms in tokenized_data:\n",
        "    pp_text = remove_stopwords(tknzd_sms)\n",
        "    pp_text = remove_non_alpha(pp_text)\n",
        "    pp_text = lemmatise(pp_text)\n",
        "    pp_data.append(pp_text)\n",
        "  return pp_data\n",
        "\n",
        "\n",
        "for i in range(len(Text_test)):\n",
        "  Text_test[i] = Negation(word_tokenize(Text_test[i].lower()))\n",
        "\n",
        "for i in range(len(Text_train)):\n",
        "  Text_train[i] = Negation(word_tokenize(Text_train[i].lower()))\n",
        "\n",
        "preprocessed_data_train = preprocess(Text_train)\n",
        "Train = preprocessed_data_train\n",
        "preprocessed_data_test = preprocess(Text_test)\n",
        "Test = preprocessed_data_test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cP9PEa3igYZl"
      },
      "source": [
        "In order to make our data more easily accessible, we'll transform our XMLs into CSVs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Evo1m-tKHo2Y"
      },
      "outputs": [],
      "source": [
        "rows_train = []\n",
        "rows_train = {         \"text\": Text_train,\n",
        "        }\n",
        " \n",
        "df_train = pd.DataFrame(rows_train)\n",
        "for i in range(len(row_text_train)):\n",
        "  row = []\n",
        "  row = {\n",
        "      row_text_train[i][0] : row_text_train[i][1:],\n",
        "      }\n",
        "  ex = pd.DataFrame(row)\n",
        "  df_train = pd.concat([df_train,ex],axis =1)\n",
        "# Writing dataframe to csv\n",
        "df_train.to_csv('training_set.csv')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ayYZCqhKNqI"
      },
      "outputs": [],
      "source": [
        "rows_test = []\n",
        "rows_test = {         \"text\": Text_test,\n",
        "        }\n",
        " \n",
        "df_test = pd.DataFrame(rows_test)\n",
        "for i in range(len(row_text_test)):\n",
        "  row = []\n",
        "  row = {\n",
        "      row_text_test[i][0] : row_text_test[i][1:],\n",
        "      }\n",
        "  ex1 = pd.DataFrame(row)\n",
        "  df_test = pd.concat([df_test,ex1],axis =1)\n",
        "# Writing dataframe to csv\n",
        "df_test.to_csv('testing_set.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 664
        },
        "id": "sofxlWpqKfeK",
        "outputId": "d2edce22-2c51-4cbf-f1f3-7b91138c1969"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-0fbd6007-558f-4009-8b3c-8c7414ac9597\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>BATTERY#OPERATION_PERFORMANCE#negative</th>\n",
              "      <th>BATTERY#OPERATION_PERFORMANCE#positive</th>\n",
              "      <th>BATTERY#QUALITY#negative</th>\n",
              "      <th>COMPANY#GENERAL#negative</th>\n",
              "      <th>COMPANY#GENERAL#positive</th>\n",
              "      <th>CPU#OPERATION_PERFORMANCE#positive</th>\n",
              "      <th>DISPLAY#DESIGN_FEATURES#negative</th>\n",
              "      <th>DISPLAY#DESIGN_FEATURES#neutral</th>\n",
              "      <th>DISPLAY#DESIGN_FEATURES#positive</th>\n",
              "      <th>...</th>\n",
              "      <th>SOFTWARE#OPERATION_PERFORMANCE#negative</th>\n",
              "      <th>SOFTWARE#OPERATION_PERFORMANCE#positive</th>\n",
              "      <th>SOFTWARE#QUALITY#positive</th>\n",
              "      <th>SOFTWARE#USABILITY#negative</th>\n",
              "      <th>SOFTWARE#USABILITY#positive</th>\n",
              "      <th>SUPPORT#PRICE#negative</th>\n",
              "      <th>SUPPORT#QUALITY#negative</th>\n",
              "      <th>SUPPORT#QUALITY#neutral</th>\n",
              "      <th>SUPPORT#QUALITY#positive</th>\n",
              "      <th>WARRANTY#GENERAL#positive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[well, ,, my, first, apple, computer, and, i, ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[works, well, ,, fast, and, no, reboots, .]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[waiting, to, install, ms, office, and, see, h...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[have, always, been, a, pc, guy, ,, but, decid...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[glad, i, did, so, far, .]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 106 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0fbd6007-558f-4009-8b3c-8c7414ac9597')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0fbd6007-558f-4009-8b3c-8c7414ac9597 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0fbd6007-558f-4009-8b3c-8c7414ac9597');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                text  \\\n",
              "0  [well, ,, my, first, apple, computer, and, i, ...   \n",
              "1        [works, well, ,, fast, and, no, reboots, .]   \n",
              "2  [waiting, to, install, ms, office, and, see, h...   \n",
              "3  [have, always, been, a, pc, guy, ,, but, decid...   \n",
              "4                         [glad, i, did, so, far, .]   \n",
              "\n",
              "   BATTERY#OPERATION_PERFORMANCE#negative  \\\n",
              "0                                       0   \n",
              "1                                       0   \n",
              "2                                       0   \n",
              "3                                       0   \n",
              "4                                       0   \n",
              "\n",
              "   BATTERY#OPERATION_PERFORMANCE#positive  BATTERY#QUALITY#negative  \\\n",
              "0                                       0                         0   \n",
              "1                                       0                         0   \n",
              "2                                       0                         0   \n",
              "3                                       0                         0   \n",
              "4                                       0                         0   \n",
              "\n",
              "   COMPANY#GENERAL#negative  COMPANY#GENERAL#positive  \\\n",
              "0                         0                         0   \n",
              "1                         0                         0   \n",
              "2                         0                         0   \n",
              "3                         0                         0   \n",
              "4                         0                         1   \n",
              "\n",
              "   CPU#OPERATION_PERFORMANCE#positive  DISPLAY#DESIGN_FEATURES#negative  \\\n",
              "0                                   0                                 0   \n",
              "1                                   0                                 0   \n",
              "2                                   0                                 0   \n",
              "3                                   0                                 0   \n",
              "4                                   0                                 0   \n",
              "\n",
              "   DISPLAY#DESIGN_FEATURES#neutral  DISPLAY#DESIGN_FEATURES#positive  ...  \\\n",
              "0                                0                                 0  ...   \n",
              "1                                0                                 0  ...   \n",
              "2                                0                                 0  ...   \n",
              "3                                0                                 0  ...   \n",
              "4                                0                                 0  ...   \n",
              "\n",
              "   SOFTWARE#OPERATION_PERFORMANCE#negative  \\\n",
              "0                                        0   \n",
              "1                                        0   \n",
              "2                                        0   \n",
              "3                                        0   \n",
              "4                                        0   \n",
              "\n",
              "   SOFTWARE#OPERATION_PERFORMANCE#positive  SOFTWARE#QUALITY#positive  \\\n",
              "0                                        0                          0   \n",
              "1                                        0                          0   \n",
              "2                                        0                          0   \n",
              "3                                        0                          0   \n",
              "4                                        0                          0   \n",
              "\n",
              "   SOFTWARE#USABILITY#negative  SOFTWARE#USABILITY#positive  \\\n",
              "0                            0                            0   \n",
              "1                            0                            0   \n",
              "2                            0                            0   \n",
              "3                            0                            0   \n",
              "4                            0                            0   \n",
              "\n",
              "   SUPPORT#PRICE#negative  SUPPORT#QUALITY#negative  SUPPORT#QUALITY#neutral  \\\n",
              "0                       0                         0                        0   \n",
              "1                       0                         0                        0   \n",
              "2                       0                         0                        0   \n",
              "3                       0                         0                        0   \n",
              "4                       0                         0                        0   \n",
              "\n",
              "   SUPPORT#QUALITY#positive  WARRANTY#GENERAL#positive  \n",
              "0                         0                          0  \n",
              "1                         0                          0  \n",
              "2                         0                          0  \n",
              "3                         0                          0  \n",
              "4                         0                          0  \n",
              "\n",
              "[5 rows x 106 columns]"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_test.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dw_D-izLw7i1"
      },
      "source": [
        "Our Model\n",
        "\n",
        "By testing different models for **Naive Bayes** and **Logistic Regression**, I have observed that **ComplementNB** works the best for my data sets. **Complement Naive Bayes** is significantly better than the other models as it is particularily suited for imbalanced data and for classes which are not mutually exclusive. **SVC** also works as well as **ComplementNB** but the performance time is a significant disadvantage in this case (it took around 2min to compile).\n",
        "\n",
        "For problem transformation from multi-label to multi-class, there are 4 methods:\n",
        "1. Binary relevance\n",
        "2. One vs. Rest\n",
        "3. Classifier chains\n",
        "4. Label powerset\n",
        "\n",
        "**Binary relevance** and **One vs. Rest** are not the best methods for my model as they treat each label independently, whereas our labels are not mutually exclusive. As we have 105 diffrent tags, **Label powerset** will lead to combinatorial explosion and thus computational infeasibility because it considers each unique label combinations found in the data as a single label.\n",
        "\n",
        "On the other hand, **Classifier Chain** creates a chain of binary classifiers C0, C1, . . . , Cn, where the first classifier is built using the input data and the following classifiers are trained using the combined inputs and the previous classifiers in the given chain. This way the method can take into account label correlations. This is a sequential process where an output of one classifier is used as the input of the next classifier in the chain.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPQIxShawOt1"
      },
      "source": [
        "Feature Selection\n",
        "\n",
        "We'll implement two different ways of selecting features from our data and we'll see which one works better for our algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hm5KydtgnNY"
      },
      "source": [
        "# 1. Mutual Information (TF-IDF)\n",
        "\n",
        "\n",
        "We train our first model using TF-IDF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOvHJ1c4I2yV"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB , ComplementNB , BernoulliNB , GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# we fit the tfidf vectorizer with our train data\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(Train)\n",
        "\n",
        "# we now use the same vectorizer used and fit with our train data with test set\n",
        "X_test_tfidf = tfidf_vectorizer.transform(Test) \n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0cVJ0u_JUkd"
      },
      "outputs": [],
      "source": [
        "y_train = (df_train[df_train.columns[1:]])\n",
        "y_test = (df_test[df_test.columns[1:]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o38IMY7JEXuY",
        "outputId": "75839063-b8c5-41c8-e029-a9ef47163b40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Classification Report\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         2\n",
            "           1       0.47      0.53      0.50        17\n",
            "           2       0.00      0.00      0.00         5\n",
            "           3       0.20      0.09      0.13        22\n",
            "           4       0.08      0.06      0.07        16\n",
            "           5       0.00      0.00      0.00         2\n",
            "           6       0.00      0.00      0.00         4\n",
            "           7       0.00      0.00      0.00         2\n",
            "           8       0.00      0.00      0.00         7\n",
            "           9       0.00      0.00      0.00         1\n",
            "          10       0.17      0.33      0.22         3\n",
            "          11       0.00      0.00      0.00         3\n",
            "          12       0.00      0.00      0.00         2\n",
            "          13       0.00      0.00      0.00         3\n",
            "          14       0.00      0.00      0.00         5\n",
            "          15       0.00      0.00      0.00         1\n",
            "          16       0.43      0.21      0.29        14\n",
            "          17       0.00      0.00      0.00         3\n",
            "          18       0.00      0.00      0.00         1\n",
            "          19       0.00      0.00      0.00         1\n",
            "          20       0.00      0.00      0.00         1\n",
            "          21       0.00      0.00      0.00         1\n",
            "          22       0.00      0.00      0.00         1\n",
            "          23       0.00      0.00      0.00         1\n",
            "          24       0.00      0.00      0.00         3\n",
            "          25       0.00      0.00      0.00         1\n",
            "          26       0.00      0.00      0.00        10\n",
            "          27       0.00      0.00      0.00         1\n",
            "          28       0.00      0.00      0.00         1\n",
            "          29       0.33      0.17      0.22         6\n",
            "          30       0.00      0.00      0.00         2\n",
            "          31       0.00      0.00      0.00         8\n",
            "          32       0.00      0.00      0.00         1\n",
            "          33       0.00      0.00      0.00         5\n",
            "          34       0.00      0.00      0.00         2\n",
            "          35       0.00      0.00      0.00         2\n",
            "          36       0.00      0.00      0.00         1\n",
            "          37       0.00      0.00      0.00         3\n",
            "          38       0.00      0.00      0.00         8\n",
            "          39       0.00      0.00      0.00         5\n",
            "          40       0.29      0.31      0.30        16\n",
            "          41       0.00      0.00      0.00         8\n",
            "          42       0.33      0.29      0.31        49\n",
            "          43       0.16      0.23      0.18        31\n",
            "          44       0.00      0.00      0.00         2\n",
            "          45       0.38      0.59      0.46       125\n",
            "          46       0.25      0.08      0.12        13\n",
            "          47       0.00      0.00      0.00         3\n",
            "          48       0.20      0.22      0.21        18\n",
            "          49       0.41      0.33      0.37        21\n",
            "          50       0.00      0.00      0.00         1\n",
            "          51       0.54      0.46      0.49        48\n",
            "          52       0.00      0.00      0.00         2\n",
            "          53       0.50      1.00      0.67         4\n",
            "          54       0.00      0.00      0.00        11\n",
            "          55       0.00      0.00      0.00         3\n",
            "          56       0.00      0.00      0.00        11\n",
            "          57       0.10      0.12      0.11        17\n",
            "          58       0.00      0.00      0.00         1\n",
            "          59       0.47      0.24      0.32        29\n",
            "          60       0.00      0.00      0.00         8\n",
            "          61       0.00      0.00      0.00         2\n",
            "          62       0.26      0.17      0.20        36\n",
            "          63       0.00      0.00      0.00         4\n",
            "          64       0.00      0.00      0.00         4\n",
            "          65       0.00      0.00      0.00         1\n",
            "          66       0.00      0.00      0.00         3\n",
            "          67       0.00      0.00      0.00         2\n",
            "          68       0.00      0.00      0.00         1\n",
            "          69       0.00      0.00      0.00         3\n",
            "          70       0.00      0.00      0.00         2\n",
            "          71       0.00      0.00      0.00        11\n",
            "          72       0.00      0.00      0.00         1\n",
            "          73       0.00      0.00      0.00         1\n",
            "          74       0.00      0.00      0.00         3\n",
            "          75       0.00      0.00      0.00         1\n",
            "          76       0.00      0.00      0.00         1\n",
            "          77       0.00      0.00      0.00         1\n",
            "          78       0.00      0.00      0.00         3\n",
            "          79       1.00      0.25      0.40         4\n",
            "          80       0.00      0.00      0.00         1\n",
            "          81       0.00      0.00      0.00         4\n",
            "          82       0.00      0.00      0.00         2\n",
            "          83       0.00      0.00      0.00         7\n",
            "          84       0.00      0.00      0.00         1\n",
            "          85       0.00      0.00      0.00         1\n",
            "          86       0.00      0.00      0.00         2\n",
            "          87       0.00      0.00      0.00         5\n",
            "          88       0.00      0.00      0.00         2\n",
            "          89       0.00      0.00      0.00         8\n",
            "          90       0.00      0.00      0.00         1\n",
            "          91       0.00      0.00      0.00         1\n",
            "          92       0.00      0.00      0.00         1\n",
            "          93       0.00      0.00      0.00         1\n",
            "          94       0.00      0.00      0.00         4\n",
            "          95       0.00      0.00      0.00         1\n",
            "          96       0.00      0.00      0.00         1\n",
            "          97       0.00      0.00      0.00         1\n",
            "          98       0.00      0.00      0.00         3\n",
            "          99       0.00      0.00      0.00         4\n",
            "         100       0.00      0.00      0.00         2\n",
            "         101       0.21      0.22      0.21        27\n",
            "         102       0.00      0.00      0.00         1\n",
            "         103       0.00      0.00      0.00         5\n",
            "         104       0.00      0.00      0.00         1\n",
            "\n",
            "   micro avg       0.27      0.23      0.25       769\n",
            "   macro avg       0.06      0.06      0.06       769\n",
            "weighted avg       0.22      0.23      0.22       769\n",
            " samples avg       0.20      0.18      0.18       769\n",
            "\n"
          ]
        }
      ],
      "source": [
        "classifier_tfidf = ClassifierChain(ComplementNB())\n",
        "classifier_tfidf.fit(X_train_tfidf, y_train)\n",
        "predictions_tfidf = classifier_tfidf.predict(X_test_tfidf)\n",
        "\n",
        "cr_tfidf = classification_report(y_test, predictions_tfidf,zero_division = 0)\n",
        "print(\"\\n\\nClassification Report\\n\")\n",
        "print(cr_tfidf)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "978UTgcBgshM"
      },
      "source": [
        "# 2. Word Frequency\n",
        "We train our second model using CountVectorizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtTdshdN3qyi",
        "outputId": "7d8dd8f8-5fd3-4928-ca7b-9786ba29e72a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Classification Report\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         2\n",
            "           1       0.27      0.53      0.36        17\n",
            "           2       0.00      0.00      0.00         5\n",
            "           3       0.14      0.14      0.14        22\n",
            "           4       0.09      0.12      0.11        16\n",
            "           5       0.00      0.00      0.00         2\n",
            "           6       0.00      0.00      0.00         4\n",
            "           7       0.00      0.00      0.00         2\n",
            "           8       0.00      0.00      0.00         7\n",
            "           9       0.00      0.00      0.00         1\n",
            "          10       0.09      0.33      0.14         3\n",
            "          11       0.00      0.00      0.00         3\n",
            "          12       0.00      0.00      0.00         2\n",
            "          13       0.00      0.00      0.00         3\n",
            "          14       0.20      0.20      0.20         5\n",
            "          15       0.00      0.00      0.00         1\n",
            "          16       0.16      0.36      0.22        14\n",
            "          17       0.00      0.00      0.00         3\n",
            "          18       0.00      0.00      0.00         1\n",
            "          19       0.00      0.00      0.00         1\n",
            "          20       0.00      0.00      0.00         1\n",
            "          21       0.00      0.00      0.00         1\n",
            "          22       0.00      0.00      0.00         1\n",
            "          23       0.00      0.00      0.00         1\n",
            "          24       0.00      0.00      0.00         3\n",
            "          25       0.00      0.00      0.00         1\n",
            "          26       0.14      0.10      0.12        10\n",
            "          27       0.00      0.00      0.00         1\n",
            "          28       0.00      0.00      0.00         1\n",
            "          29       0.20      0.33      0.25         6\n",
            "          30       0.00      0.00      0.00         2\n",
            "          31       0.00      0.00      0.00         8\n",
            "          32       0.00      0.00      0.00         1\n",
            "          33       0.00      0.00      0.00         5\n",
            "          34       0.00      0.00      0.00         2\n",
            "          35       0.00      0.00      0.00         2\n",
            "          36       0.00      0.00      0.00         1\n",
            "          37       0.10      0.33      0.15         3\n",
            "          38       0.09      0.12      0.11         8\n",
            "          39       0.11      0.20      0.14         5\n",
            "          40       0.20      0.31      0.24        16\n",
            "          41       0.00      0.00      0.00         8\n",
            "          42       0.23      0.39      0.29        49\n",
            "          43       0.15      0.26      0.19        31\n",
            "          44       0.00      0.00      0.00         2\n",
            "          45       0.38      0.60      0.47       125\n",
            "          46       0.00      0.00      0.00        13\n",
            "          47       0.00      0.00      0.00         3\n",
            "          48       0.11      0.17      0.13        18\n",
            "          49       0.32      0.29      0.30        21\n",
            "          50       0.00      0.00      0.00         1\n",
            "          51       0.37      0.50      0.42        48\n",
            "          52       0.00      0.00      0.00         2\n",
            "          53       0.40      1.00      0.57         4\n",
            "          54       0.00      0.00      0.00        11\n",
            "          55       0.00      0.00      0.00         3\n",
            "          56       0.30      0.27      0.29        11\n",
            "          57       0.16      0.29      0.21        17\n",
            "          58       0.00      0.00      0.00         1\n",
            "          59       0.38      0.21      0.27        29\n",
            "          60       0.00      0.00      0.00         8\n",
            "          61       0.00      0.00      0.00         2\n",
            "          62       0.20      0.22      0.21        36\n",
            "          63       0.00      0.00      0.00         4\n",
            "          64       0.14      0.50      0.22         4\n",
            "          65       0.00      0.00      0.00         1\n",
            "          66       0.20      0.33      0.25         3\n",
            "          67       0.00      0.00      0.00         2\n",
            "          68       0.00      0.00      0.00         1\n",
            "          69       0.00      0.00      0.00         3\n",
            "          70       0.00      0.00      0.00         2\n",
            "          71       0.40      0.18      0.25        11\n",
            "          72       0.00      0.00      0.00         1\n",
            "          73       0.25      1.00      0.40         1\n",
            "          74       0.00      0.00      0.00         3\n",
            "          75       0.50      1.00      0.67         1\n",
            "          76       0.00      0.00      0.00         1\n",
            "          77       0.00      0.00      0.00         1\n",
            "          78       0.00      0.00      0.00         3\n",
            "          79       0.00      0.00      0.00         4\n",
            "          80       0.00      0.00      0.00         1\n",
            "          81       0.00      0.00      0.00         4\n",
            "          82       0.00      0.00      0.00         2\n",
            "          83       0.00      0.00      0.00         7\n",
            "          84       0.00      0.00      0.00         1\n",
            "          85       0.00      0.00      0.00         1\n",
            "          86       0.00      0.00      0.00         2\n",
            "          87       0.00      0.00      0.00         5\n",
            "          88       0.00      0.00      0.00         2\n",
            "          89       0.00      0.00      0.00         8\n",
            "          90       0.00      0.00      0.00         1\n",
            "          91       0.00      0.00      0.00         1\n",
            "          92       0.00      0.00      0.00         1\n",
            "          93       0.00      0.00      0.00         1\n",
            "          94       0.00      0.00      0.00         4\n",
            "          95       0.33      1.00      0.50         1\n",
            "          96       0.00      0.00      0.00         1\n",
            "          97       0.00      0.00      0.00         1\n",
            "          98       0.00      0.00      0.00         3\n",
            "          99       0.00      0.00      0.00         4\n",
            "         100       0.00      0.00      0.00         2\n",
            "         101       0.22      0.33      0.26        27\n",
            "         102       0.00      0.00      0.00         1\n",
            "         103       0.00      0.00      0.00         5\n",
            "         104       0.00      0.00      0.00         1\n",
            "\n",
            "   micro avg       0.19      0.27      0.23       769\n",
            "   macro avg       0.07      0.11      0.08       769\n",
            "weighted avg       0.19      0.27      0.22       769\n",
            " samples avg       0.20      0.21      0.19       769\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "count_vectorizer = CountVectorizer()\n",
        "\n",
        "# we fit the count vectorizer with our train data\n",
        "X_train_counts = count_vectorizer.fit_transform(Train)\n",
        "\n",
        "# we now use the same vectorizer used and fit with our train data with test set\n",
        "X_test_counts = count_vectorizer.transform(Test) \n",
        "\n",
        "\n",
        "classifier_count = ClassifierChain(ComplementNB())\n",
        "classifier_count.fit(X_train_counts, y_train)\n",
        "predictions_count = classifier_count.predict(X_test_counts)\n",
        "\n",
        "\n",
        "cr_count = classification_report(y_test, predictions_count,zero_division = 0)\n",
        "print(\"\\n\\nClassification Report\\n\")\n",
        "print(cr_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fxbLdLL4e8x"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "TF-IDF works slightly better than CountVectorizer.\n",
        "\n",
        " By looking at the classification report we notice that one of our biggest problems is the unbalanced data (many sentences have the LAPTOP#GENERAL#positive tag). This clearly affects the model as there are not enough examples for each category to learn from. One way of improving our model is to try and generate syntetic data to make our data more balanced. Or we could add weights to our classes to make the smaller ones more important. \n",
        " \n",
        "  We can also see that negative or neutral tags have a lower f1 score comapred to the positive tags. One reason is the low number of examples for those tags but also negation plays an important role in this. A better method of handling negation might help in this case.\n",
        "\n",
        "Implementing a Hierarchical model to handle multi-labeling and the polarity of the labels could also be an interesting approach to this problem. Another interesting idea would be to make two models: we create a model which handels our aspect analysis task then we create a model which takes our predicted labels and generates their polarity."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
